{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "import nnfs\n",
    "nnfs.init()\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from input ones, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify the original variable,\n",
    "        # letâ€™s make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    " # Forward pass\n",
    " def forward(self, inputs):\n",
    " # Get unnormalized probabilities\n",
    "  exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    " # Normalize them for each sample\n",
    "  probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
    "  self.output = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    " # Calculates the data and regularization losses\n",
    " # given model output and ground truth values\n",
    " def calculate(self, output, y):\n",
    "  # Calculate sample losses\n",
    "  sample_losses = self.forward(output, y)\n",
    "  # Calculate mean loss\n",
    "  data_loss = np.mean(sample_losses)\n",
    "  # Return loss\n",
    "  return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - \\\n",
    "                             self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - \\\n",
    "                           self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.360, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.443, loss: 1.053, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.550, loss: 0.955, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.640, loss: 0.763, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.647, loss: 0.732, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.687, loss: 0.666, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.757, loss: 0.564, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.787, loss: 0.513, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.730, loss: 0.604, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.833, loss: 0.442, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.837, loss: 0.447, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.780, loss: 0.534, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.867, loss: 0.390, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.877, loss: 0.361, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.850, loss: 0.411, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.840, loss: 0.395, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.883, loss: 0.311, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.893, loss: 0.301, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.817, loss: 0.483, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.893, loss: 0.310, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.897, loss: 0.282, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.910, loss: 0.262, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.913, loss: 0.249, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.913, loss: 0.238, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.913, loss: 0.229, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.917, loss: 0.222, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.920, loss: 0.214, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.920, loss: 0.207, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.920, loss: 0.202, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.920, loss: 0.196, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.923, loss: 0.192, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.927, loss: 0.187, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.930, loss: 0.184, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.930, loss: 0.180, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.940, loss: 0.172, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.940, loss: 0.169, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.940, loss: 0.167, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.940, loss: 0.164, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.940, loss: 0.162, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.940, loss: 0.160, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.940, loss: 0.158, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.940, loss: 0.157, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.940, loss: 0.155, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.940, loss: 0.154, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.940, loss: 0.152, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.940, loss: 0.150, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.940, loss: 0.149, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.940, loss: 0.148, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.943, loss: 0.147, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.947, loss: 0.146, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.947, loss: 0.145, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.943, loss: 0.144, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.947, loss: 0.143, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.947, loss: 0.142, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.947, loss: 0.142, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.947, loss: 0.141, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.947, loss: 0.140, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.947, loss: 0.139, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.943, loss: 0.139, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.943, loss: 0.138, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.943, loss: 0.138, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.943, loss: 0.137, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.947, loss: 0.136, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.943, loss: 0.136, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.947, loss: 0.135, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.947, loss: 0.135, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.947, loss: 0.134, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.947, loss: 0.134, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.947, loss: 0.133, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.947, loss: 0.133, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.947, loss: 0.132, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.947, loss: 0.132, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.947, loss: 0.132, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.947, loss: 0.131, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.947, loss: 0.131, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.947, loss: 0.130, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.947, loss: 0.130, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.947, loss: 0.129, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.947, loss: 0.129, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.947, loss: 0.128, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.950, loss: 0.128, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.950, loss: 0.127, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.950, loss: 0.127, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.950, loss: 0.127, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.950, loss: 0.126, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.953, loss: 0.126, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.950, loss: 0.125, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.950, loss: 0.125, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.953, loss: 0.125, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.957, loss: 0.124, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.957, loss: 0.124, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.953, loss: 0.124, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.953, loss: 0.124, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.957, loss: 0.123, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.953, loss: 0.123, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.957, loss: 0.123, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.957, loss: 0.122, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.957, loss: 0.122, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.957, loss: 0.122, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.957, loss: 0.121, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.957, loss: 0.121, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming the necessary classes (Layer_Dense, Activation_ReLU, \n",
    "# Activation_Softmax_Loss_CategoricalCrossentropy, Optimizer_SGD, and spiral_data) are defined elsewhere\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=1e-3, momentum=0.9)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class optimizer_ADAM:\n",
    "#     def __init__(self,decay=0.,leaning_rate=0.001,beta_1=0.9,beta_2=0.999,epsion=1e-7):\n",
    "#         self.iteration=0\n",
    "#         self.beta_1=beta_1\n",
    "#         self.beta_2=beta_2\n",
    "#         self.leaning_rate=leaning_rate\n",
    "#         self.current_learningrate=leaning_rate\n",
    "#         self.epsion=epsion\n",
    "#         self.decay=decay\n",
    "#     def pre_update(self):\n",
    "#         if self.decay:\n",
    "#          self.current_learningrate=self.leaning_rate / (1+self.decay*self.iteration)\n",
    "#     def update(self,layer):\n",
    "#        if not hasattr('layer','weight_cache'):\n",
    "#           layer.weight_momentum=np.zeros_like(layer.weights)\n",
    "#           layer.weight_cache=np.zeros_like(layer.weights)\n",
    "#           layer.biases_momentum=np.zeros_like(layer.biases)\n",
    "#           layer.biases_cache=np.zeros_like(layer.biases)\n",
    "\n",
    "#         layer.weight_momentum=self.beta_1*layer.weight_momentum+(1-self.beta_1)*layer.dweights\n",
    "         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.350, loss: 1.099, lr: 0.001\n",
      "epoch: 100, acc: 0.453, loss: 1.056, lr: 0.0009099181073703368\n",
      "epoch: 200, acc: 0.450, loss: 1.052, lr: 0.0008340283569641367\n",
      "epoch: 300, acc: 0.457, loss: 1.051, lr: 0.0007698229407236335\n",
      "epoch: 400, acc: 0.453, loss: 1.049, lr: 0.0007147962830593281\n",
      "epoch: 500, acc: 0.457, loss: 1.047, lr: 0.0006671114076050701\n",
      "epoch: 600, acc: 0.453, loss: 1.044, lr: 0.0006253908692933083\n",
      "epoch: 700, acc: 0.443, loss: 1.041, lr: 0.0005885815185403178\n",
      "epoch: 800, acc: 0.463, loss: 1.038, lr: 0.0005558643690939412\n",
      "epoch: 900, acc: 0.467, loss: 1.034, lr: 0.000526592943654555\n",
      "epoch: 1000, acc: 0.477, loss: 1.029, lr: 0.0005002501250625312\n",
      "epoch: 1100, acc: 0.477, loss: 1.024, lr: 0.0004764173415912339\n",
      "epoch: 1200, acc: 0.467, loss: 1.019, lr: 0.0004547521600727604\n",
      "epoch: 1300, acc: 0.477, loss: 1.015, lr: 0.00043497172683775554\n",
      "epoch: 1400, acc: 0.490, loss: 1.010, lr: 0.00041684035014589413\n",
      "epoch: 1500, acc: 0.500, loss: 1.006, lr: 0.0004001600640256102\n",
      "epoch: 1600, acc: 0.500, loss: 1.001, lr: 0.0003847633705271258\n",
      "epoch: 1700, acc: 0.503, loss: 0.998, lr: 0.0003705075954057058\n",
      "epoch: 1800, acc: 0.503, loss: 0.994, lr: 0.0003572704537334763\n",
      "epoch: 1900, acc: 0.507, loss: 0.990, lr: 0.0003449465332873405\n",
      "epoch: 2000, acc: 0.503, loss: 0.986, lr: 0.00033344448149383126\n",
      "epoch: 2100, acc: 0.503, loss: 0.982, lr: 0.00032268473701193933\n",
      "epoch: 2200, acc: 0.510, loss: 0.979, lr: 0.0003125976867771179\n",
      "epoch: 2300, acc: 0.507, loss: 0.975, lr: 0.00030312215822976665\n",
      "epoch: 2400, acc: 0.510, loss: 0.972, lr: 0.00029420417769932336\n",
      "epoch: 2500, acc: 0.507, loss: 0.969, lr: 0.0002857959416976279\n",
      "epoch: 2600, acc: 0.503, loss: 0.965, lr: 0.0002778549597110308\n",
      "epoch: 2700, acc: 0.507, loss: 0.962, lr: 0.0002703433360367667\n",
      "epoch: 2800, acc: 0.507, loss: 0.959, lr: 0.00026322716504343247\n",
      "epoch: 2900, acc: 0.510, loss: 0.956, lr: 0.00025647601949217746\n",
      "epoch: 3000, acc: 0.513, loss: 0.954, lr: 0.00025006251562890725\n",
      "epoch: 3100, acc: 0.513, loss: 0.951, lr: 0.00024396194193705782\n",
      "epoch: 3200, acc: 0.507, loss: 0.948, lr: 0.00023815194093831867\n",
      "epoch: 3300, acc: 0.503, loss: 0.946, lr: 0.00023261223540358225\n",
      "epoch: 3400, acc: 0.510, loss: 0.944, lr: 0.00022732439190725165\n",
      "epoch: 3500, acc: 0.513, loss: 0.941, lr: 0.00022227161591464767\n",
      "epoch: 3600, acc: 0.517, loss: 0.939, lr: 0.00021743857360295715\n",
      "epoch: 3700, acc: 0.510, loss: 0.937, lr: 0.00021281123643328368\n",
      "epoch: 3800, acc: 0.513, loss: 0.935, lr: 0.00020837674515524068\n",
      "epoch: 3900, acc: 0.517, loss: 0.933, lr: 0.00020412329046744235\n",
      "epoch: 4000, acc: 0.520, loss: 0.931, lr: 0.0002000400080016003\n",
      "epoch: 4100, acc: 0.523, loss: 0.930, lr: 0.00019611688566385565\n",
      "epoch: 4200, acc: 0.523, loss: 0.928, lr: 0.00019234468166955186\n",
      "epoch: 4300, acc: 0.527, loss: 0.926, lr: 0.00018871485185884126\n",
      "epoch: 4400, acc: 0.530, loss: 0.924, lr: 0.00018521948508983145\n",
      "epoch: 4500, acc: 0.527, loss: 0.923, lr: 0.00018185124568103294\n",
      "epoch: 4600, acc: 0.527, loss: 0.921, lr: 0.0001786033220217896\n",
      "epoch: 4700, acc: 0.530, loss: 0.919, lr: 0.00017546938059308652\n",
      "epoch: 4800, acc: 0.530, loss: 0.918, lr: 0.0001724435247456458\n",
      "epoch: 4900, acc: 0.527, loss: 0.916, lr: 0.00016952025767079165\n",
      "epoch: 5000, acc: 0.530, loss: 0.915, lr: 0.00016669444907484583\n",
      "epoch: 5100, acc: 0.533, loss: 0.913, lr: 0.00016396130513198883\n",
      "epoch: 5200, acc: 0.537, loss: 0.912, lr: 0.00016131634134537829\n",
      "epoch: 5300, acc: 0.537, loss: 0.910, lr: 0.00015875535799333228\n",
      "epoch: 5400, acc: 0.540, loss: 0.909, lr: 0.0001562744178777934\n",
      "epoch: 5500, acc: 0.540, loss: 0.907, lr: 0.00015386982612709647\n",
      "epoch: 5600, acc: 0.540, loss: 0.906, lr: 0.00015153811183512653\n",
      "epoch: 5700, acc: 0.540, loss: 0.905, lr: 0.00014927601134497688\n",
      "epoch: 5800, acc: 0.543, loss: 0.903, lr: 0.00014708045300779526\n",
      "epoch: 5900, acc: 0.547, loss: 0.902, lr: 0.00014494854326714017\n",
      "epoch: 6000, acc: 0.547, loss: 0.901, lr: 0.00014287755393627662\n",
      "epoch: 6100, acc: 0.550, loss: 0.899, lr: 0.00014086491055078179\n",
      "epoch: 6200, acc: 0.550, loss: 0.898, lr: 0.00013890818169190168\n",
      "epoch: 6300, acc: 0.557, loss: 0.897, lr: 0.00013700506918755992\n",
      "epoch: 6400, acc: 0.557, loss: 0.896, lr: 0.00013515339910798757\n",
      "epoch: 6500, acc: 0.557, loss: 0.895, lr: 0.00013335111348179756\n",
      "epoch: 6600, acc: 0.557, loss: 0.894, lr: 0.00013159626266614027\n",
      "epoch: 6700, acc: 0.557, loss: 0.893, lr: 0.000129886998311469\n",
      "epoch: 6800, acc: 0.557, loss: 0.892, lr: 0.00012822156686754713\n",
      "epoch: 6900, acc: 0.557, loss: 0.891, lr: 0.000126598303582732\n",
      "epoch: 7000, acc: 0.557, loss: 0.889, lr: 0.00012501562695336915\n",
      "epoch: 7100, acc: 0.557, loss: 0.888, lr: 0.00012347203358439312\n",
      "epoch: 7200, acc: 0.557, loss: 0.887, lr: 0.00012196609342602758\n",
      "epoch: 7300, acc: 0.557, loss: 0.885, lr: 0.00012049644535486204\n",
      "epoch: 7400, acc: 0.553, loss: 0.884, lr: 0.00011906179307060363\n",
      "epoch: 7500, acc: 0.550, loss: 0.882, lr: 0.00011766090128250381\n",
      "epoch: 7600, acc: 0.550, loss: 0.881, lr: 0.0001162925921618793\n",
      "epoch: 7700, acc: 0.547, loss: 0.879, lr: 0.00011495574203931487\n",
      "epoch: 7800, acc: 0.543, loss: 0.878, lr: 0.00011364927832708264\n",
      "epoch: 7900, acc: 0.543, loss: 0.877, lr: 0.00011237217664906169\n",
      "epoch: 8000, acc: 0.543, loss: 0.876, lr: 0.000111123458162018\n",
      "epoch: 8100, acc: 0.547, loss: 0.874, lr: 0.00010990218705352236\n",
      "epoch: 8200, acc: 0.547, loss: 0.873, lr: 0.00010870746820306555\n",
      "epoch: 8300, acc: 0.550, loss: 0.872, lr: 0.0001075384449940854\n",
      "epoch: 8400, acc: 0.550, loss: 0.871, lr: 0.00010639429726566655\n",
      "epoch: 8500, acc: 0.560, loss: 0.869, lr: 0.00010527423939362037\n",
      "epoch: 8600, acc: 0.560, loss: 0.868, lr: 0.00010417751849150952\n",
      "epoch: 8700, acc: 0.557, loss: 0.867, lr: 0.00010310341272296113\n",
      "epoch: 8800, acc: 0.553, loss: 0.865, lr: 0.0001020512297173181\n",
      "epoch: 8900, acc: 0.553, loss: 0.864, lr: 0.00010102030508132135\n",
      "epoch: 9000, acc: 0.553, loss: 0.863, lr: 0.00010001000100010001\n",
      "epoch: 9100, acc: 0.553, loss: 0.862, lr: 9.901970492127933e-05\n",
      "epoch: 9200, acc: 0.557, loss: 0.861, lr: 9.804882831650162e-05\n",
      "epoch: 9300, acc: 0.557, loss: 0.860, lr: 9.709680551509856e-05\n",
      "epoch: 9400, acc: 0.560, loss: 0.859, lr: 9.616309260505818e-05\n",
      "epoch: 9500, acc: 0.560, loss: 0.858, lr: 9.524716639679969e-05\n",
      "epoch: 9600, acc: 0.573, loss: 0.856, lr: 9.434852344560806e-05\n",
      "epoch: 9700, acc: 0.583, loss: 0.855, lr: 9.346667912889055e-05\n",
      "epoch: 9800, acc: 0.583, loss: 0.854, lr: 9.260116677470138e-05\n",
      "epoch: 9900, acc: 0.587, loss: 0.853, lr: 9.175153683824203e-05\n",
      "epoch: 10000, acc: 0.590, loss: 0.852, lr: 9.091735612328394e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming the necessary classes (Layer_Dense, Activation_ReLU, \n",
    "# Activation_Softmax_Loss_CategoricalCrossentropy, Optimizer_SGD, and spiral_data) are defined elsewhere\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(decay=1e-3)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.353, loss: 1.099, lr: 0.02\n",
      "epoch: 100, acc: 0.577, loss: 0.873, lr: 0.01998021958261321\n",
      "epoch: 200, acc: 0.727, loss: 0.662, lr: 0.019960279044701046\n",
      "epoch: 300, acc: 0.777, loss: 0.542, lr: 0.019940378268975763\n",
      "epoch: 400, acc: 0.830, loss: 0.451, lr: 0.01992051713662487\n",
      "epoch: 500, acc: 0.830, loss: 0.403, lr: 0.01990069552930875\n",
      "epoch: 600, acc: 0.857, loss: 0.364, lr: 0.019880913329158343\n",
      "epoch: 700, acc: 0.883, loss: 0.325, lr: 0.019861170418772778\n",
      "epoch: 800, acc: 0.883, loss: 0.296, lr: 0.019841466681217078\n",
      "epoch: 900, acc: 0.900, loss: 0.276, lr: 0.01982180200001982\n",
      "epoch: 1000, acc: 0.910, loss: 0.257, lr: 0.019802176259170884\n",
      "epoch: 1100, acc: 0.907, loss: 0.243, lr: 0.01978258934311912\n",
      "epoch: 1200, acc: 0.923, loss: 0.230, lr: 0.01976304113677013\n",
      "epoch: 1300, acc: 0.933, loss: 0.220, lr: 0.019743531525483964\n",
      "epoch: 1400, acc: 0.937, loss: 0.210, lr: 0.01972406039507293\n",
      "epoch: 1500, acc: 0.933, loss: 0.202, lr: 0.019704627631799327\n",
      "epoch: 1600, acc: 0.937, loss: 0.195, lr: 0.019685233122373254\n",
      "epoch: 1700, acc: 0.940, loss: 0.188, lr: 0.019665876753950384\n",
      "epoch: 1800, acc: 0.943, loss: 0.183, lr: 0.01964655841412981\n",
      "epoch: 1900, acc: 0.943, loss: 0.177, lr: 0.019627277990951823\n",
      "epoch: 2000, acc: 0.940, loss: 0.172, lr: 0.019608035372895814\n",
      "epoch: 2100, acc: 0.947, loss: 0.169, lr: 0.01958883044887805\n",
      "epoch: 2200, acc: 0.943, loss: 0.165, lr: 0.019569663108249594\n",
      "epoch: 2300, acc: 0.950, loss: 0.162, lr: 0.01955053324079414\n",
      "epoch: 2400, acc: 0.947, loss: 0.158, lr: 0.019531440736725945\n",
      "epoch: 2500, acc: 0.943, loss: 0.156, lr: 0.019512385486687673\n",
      "epoch: 2600, acc: 0.947, loss: 0.152, lr: 0.019493367381748363\n",
      "epoch: 2700, acc: 0.947, loss: 0.150, lr: 0.019474386313401298\n",
      "epoch: 2800, acc: 0.940, loss: 0.149, lr: 0.019455442173562\n",
      "epoch: 2900, acc: 0.950, loss: 0.145, lr: 0.019436534854566128\n",
      "epoch: 3000, acc: 0.953, loss: 0.143, lr: 0.01941766424916747\n",
      "epoch: 3100, acc: 0.953, loss: 0.141, lr: 0.019398830250535893\n",
      "epoch: 3200, acc: 0.953, loss: 0.140, lr: 0.019380032752255354\n",
      "epoch: 3300, acc: 0.950, loss: 0.139, lr: 0.01936127164832186\n",
      "epoch: 3400, acc: 0.953, loss: 0.137, lr: 0.01934254683314152\n",
      "epoch: 3500, acc: 0.953, loss: 0.135, lr: 0.019323858201528515\n",
      "epoch: 3600, acc: 0.950, loss: 0.133, lr: 0.019305205648703173\n",
      "epoch: 3700, acc: 0.950, loss: 0.132, lr: 0.01928658907028997\n",
      "epoch: 3800, acc: 0.953, loss: 0.131, lr: 0.01926800836231563\n",
      "epoch: 3900, acc: 0.950, loss: 0.130, lr: 0.019249463421207133\n",
      "epoch: 4000, acc: 0.953, loss: 0.128, lr: 0.019230954143789846\n",
      "epoch: 4100, acc: 0.950, loss: 0.129, lr: 0.019212480427285565\n",
      "epoch: 4200, acc: 0.950, loss: 0.126, lr: 0.019194042169310647\n",
      "epoch: 4300, acc: 0.947, loss: 0.128, lr: 0.019175639267874092\n",
      "epoch: 4400, acc: 0.953, loss: 0.125, lr: 0.019157271621375684\n",
      "epoch: 4500, acc: 0.953, loss: 0.124, lr: 0.0191389391286041\n",
      "epoch: 4600, acc: 0.953, loss: 0.123, lr: 0.019120641688735073\n",
      "epoch: 4700, acc: 0.953, loss: 0.122, lr: 0.019102379201329525\n",
      "epoch: 4800, acc: 0.953, loss: 0.122, lr: 0.01908415156633174\n",
      "epoch: 4900, acc: 0.953, loss: 0.121, lr: 0.01906595868406753\n",
      "epoch: 5000, acc: 0.940, loss: 0.126, lr: 0.01904780045524243\n",
      "epoch: 5100, acc: 0.950, loss: 0.121, lr: 0.019029676780939874\n",
      "epoch: 5200, acc: 0.953, loss: 0.119, lr: 0.019011587562619416\n",
      "epoch: 5300, acc: 0.953, loss: 0.121, lr: 0.01899353270211493\n",
      "epoch: 5400, acc: 0.950, loss: 0.120, lr: 0.018975512101632844\n",
      "epoch: 5500, acc: 0.953, loss: 0.118, lr: 0.018957525663750367\n",
      "epoch: 5600, acc: 0.950, loss: 0.120, lr: 0.018939573291413745\n",
      "epoch: 5700, acc: 0.950, loss: 0.118, lr: 0.018921654887936498\n",
      "epoch: 5800, acc: 0.950, loss: 0.117, lr: 0.018903770356997706\n",
      "epoch: 5900, acc: 0.953, loss: 0.116, lr: 0.018885919602640248\n",
      "epoch: 6000, acc: 0.953, loss: 0.120, lr: 0.018868102529269144\n",
      "epoch: 6100, acc: 0.950, loss: 0.117, lr: 0.018850319041649778\n",
      "epoch: 6200, acc: 0.957, loss: 0.114, lr: 0.018832569044906263\n",
      "epoch: 6300, acc: 0.950, loss: 0.115, lr: 0.018814852444519702\n",
      "epoch: 6400, acc: 0.953, loss: 0.114, lr: 0.018797169146326564\n",
      "epoch: 6500, acc: 0.950, loss: 0.114, lr: 0.01877951905651696\n",
      "epoch: 6600, acc: 0.950, loss: 0.113, lr: 0.018761902081633034\n",
      "epoch: 6700, acc: 0.953, loss: 0.113, lr: 0.018744318128567278\n",
      "epoch: 6800, acc: 0.950, loss: 0.112, lr: 0.018726767104560903\n",
      "epoch: 6900, acc: 0.953, loss: 0.112, lr: 0.018709248917202218\n",
      "epoch: 7000, acc: 0.950, loss: 0.112, lr: 0.018691763474424996\n",
      "epoch: 7100, acc: 0.953, loss: 0.111, lr: 0.018674310684506857\n",
      "epoch: 7200, acc: 0.947, loss: 0.112, lr: 0.01865689045606769\n",
      "epoch: 7300, acc: 0.953, loss: 0.111, lr: 0.01863950269806802\n",
      "epoch: 7400, acc: 0.950, loss: 0.111, lr: 0.018622147319807447\n",
      "epoch: 7500, acc: 0.957, loss: 0.112, lr: 0.018604824230923075\n",
      "epoch: 7600, acc: 0.953, loss: 0.111, lr: 0.01858753334138793\n",
      "epoch: 7700, acc: 0.947, loss: 0.110, lr: 0.018570274561509396\n",
      "epoch: 7800, acc: 0.950, loss: 0.110, lr: 0.018553047801927663\n",
      "epoch: 7900, acc: 0.953, loss: 0.109, lr: 0.018535852973614212\n",
      "epoch: 8000, acc: 0.953, loss: 0.109, lr: 0.01851868998787026\n",
      "epoch: 8100, acc: 0.950, loss: 0.110, lr: 0.018501558756325222\n",
      "epoch: 8200, acc: 0.953, loss: 0.109, lr: 0.01848445919093522\n",
      "epoch: 8300, acc: 0.950, loss: 0.108, lr: 0.018467391203981567\n",
      "epoch: 8400, acc: 0.953, loss: 0.108, lr: 0.018450354708069265\n",
      "epoch: 8500, acc: 0.953, loss: 0.106, lr: 0.018433349616125496\n",
      "epoch: 8600, acc: 0.953, loss: 0.105, lr: 0.018416375841398172\n",
      "epoch: 8700, acc: 0.953, loss: 0.104, lr: 0.01839943329745444\n",
      "epoch: 8800, acc: 0.953, loss: 0.105, lr: 0.01838252189817921\n",
      "epoch: 8900, acc: 0.957, loss: 0.104, lr: 0.018365641557773718\n",
      "epoch: 9000, acc: 0.950, loss: 0.108, lr: 0.018348792190754044\n",
      "epoch: 9100, acc: 0.957, loss: 0.103, lr: 0.0183319737119497\n",
      "epoch: 9200, acc: 0.953, loss: 0.104, lr: 0.018315186036502167\n",
      "epoch: 9300, acc: 0.953, loss: 0.103, lr: 0.018298429079863496\n",
      "epoch: 9400, acc: 0.957, loss: 0.103, lr: 0.018281702757794862\n",
      "epoch: 9500, acc: 0.957, loss: 0.102, lr: 0.018265006986365174\n",
      "epoch: 9600, acc: 0.957, loss: 0.102, lr: 0.018248341681949654\n",
      "epoch: 9700, acc: 0.957, loss: 0.102, lr: 0.018231706761228456\n",
      "epoch: 9800, acc: 0.953, loss: 0.101, lr: 0.018215102141185255\n",
      "epoch: 9900, acc: 0.957, loss: 0.100, lr: 0.018198527739105907\n",
      "epoch: 10000, acc: 0.957, loss: 0.100, lr: 0.018181983472577025\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "#optimizer = Optimizer_SGD(decay=8e-8, momentum=0.9)\n",
    "#optimizer = Optimizer_Adagrad(decay=1e-4)\n",
    "#optimizer = Optimizer_RMSprop(decay=1e-4)\n",
    "#optimizer = Optimizer_RMSprop(learning_rate=0.02, decay=1e-5,rho=0.999)\n",
    "\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay=1e-5)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
